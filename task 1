import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Step 1: Load Data
def load_data(file_path):
    """Load dataset from a CSV file."""
    return pd.read_csv(file_path)

# Step 2: Preprocessing

def preprocess_data(data):
    """Preprocess the data: handle missing values, encode categorical variables, and scale numerical variables."""
    
    # Identify categorical and numerical columns
    categorical_features = data.select_dtypes(include=['object']).columns
    numerical_features = data.select_dtypes(include=['int64', 'float64']).columns

    # Imputation for missing values
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    # Combine preprocessors in a column transformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    return preprocessor

# Step 3: Split Data
def split_data(data, target_column, test_size=0.2, random_state=42):
    """Split the data into training and testing sets."""
    X = data.drop(target_column, axis=1)
    y = data[target_column]
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

# Step 4: Build a Pipeline
def build_pipeline(preprocessor, model):
    """Build a pipeline that includes preprocessing and the model."""
    return Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

# Example Usage
if __name__ == "__main__":
    # Load dataset
    file_path = 'your_dataset.csv'  # Replace with your dataset path
    data = load_data(file_path)

    # Preprocess data
    target_column = 'target'  # Replace with your target column name
    preprocessor = preprocess_data(data)

    # Split data
    X_train, X_test, y_train, y_test = split_data(data, target_column)

    # Example model (Replace with your preferred model, e.g., RandomForestClassifier, LinearRegression, etc.)
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(random_state=42)

    # Build pipeline
    pipeline = build_pipeline(preprocessor, model)

    # Fit pipeline to the training data
    pipeline.fit(X_train, y_train)

    # Evaluate the model
    print(f"Model Score: {pipeline.score(X_test, y_test):.2f}")
